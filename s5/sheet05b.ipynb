{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb881b440faff8e8427d5a2861f39e9f",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2020) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4dec75f55c7466598fd339a39563727",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 05b: Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c56fe54c3bd16e2ad3b86af3458c32e2",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Saturday, June 06, 2020**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02ee44b2d4b919084eedabb2aaf97b26",
     "grade": false,
     "grade_id": "cell-26f189cc4f9865c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Covariance and autocorrelation matrix [5 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a45c4a71fcff8329e49db5c7132c61eb",
     "grade": false,
     "grade_id": "cell-b2aebe0aabf8db03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** Recap: What is the covariance matrix? What do the different entries of the matrix mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e38327010723cbbf66fd5dc57ad3d4e",
     "grade": true,
     "grade_id": "cell-bd9c31f2e24eac43",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The covariance matrix tells you how much one feature in your data set covariates with another feature in the data set.\n",
    "On the diagonal, you just have the variance of the individual features and the non-diagonal elements tell you how feature i covariates with feature j.\n",
    "A positive covariance value means that high values for the first feature correspond to high values for the second one. A negative covariance value on the other hand indicates that high values for the first feature correspond to low values in the second feature (0 means no correspondence at all).\n",
    "If the covariance matrix is normalized, it results in the correlation matrix with values between -1 and 1 (correlation between features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed6ca599372102f51d37c5a6a37fa48f",
     "grade": false,
     "grade_id": "cell-567b831bb3eb84cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What can the covariance matrix be used for? Name at least two applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3920ef158eef07c2fa413712f2345bf3",
     "grade": true,
     "grade_id": "cell-8594d318796af945",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "- PCA: The PCs are eigenvectors of the covariance matrix\n",
    "- Stochastic modeling in general, e.g. Cholesky decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66c77687b8c6ddf43a54df0379d15b67",
     "grade": false,
     "grade_id": "cell-6ebef2c8d8ea9ef8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Execute the next cell. You can see three data sets. Which of the three covariance matrices below belongs to which plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "758469370011aae11cf39c1e493476b4",
     "grade": false,
     "grade_id": "cell-16f29c1eaf43f3d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "factors = [0.5,1,0]\n",
    "names = [\"a\", \"b\", \"c\"]\n",
    "datasets = []\n",
    "for i, factor in enumerate(factors):\n",
    "    x = np.array(np.arange(100).tolist() + np.arange(100).tolist())\n",
    "    if factor!=0:\n",
    "        y = x * factor + (np.random.rand(len(x))-0.5)*30\n",
    "    else:\n",
    "        y = np.array([50 for _ in range(len(x))]) + (np.random.rand(len(x))-0.5)*30\n",
    "    plt.subplot(1,len(factors),i+1)\n",
    "    plt.scatter(x,y)\n",
    "    plt.title(\"Dataset \"+names[i], fontsize = 30)\n",
    "    plt.xlabel(\"x\", fontsize = 20)\n",
    "    plt.ylabel(\"y\", fontsize = 20)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(0, 100)\n",
    "    datasets.append(np.stack([x, y], axis = 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d22b9f00c123d79f113c8a61bd73640a",
     "grade": false,
     "grade_id": "cell-5d81f7209716d42a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Covariance matrix I:**\n",
    "\n",
    "| C1    |  x  |  y  |\n",
    "|-------|-----|-----|\n",
    "| **x** | 837 | 877 |\n",
    "| **y** | 877 | 989 |\n",
    "\n",
    "**Covariance matrix II:**\n",
    "\n",
    " | C2 | x | y |\n",
    " | ---|---|---|\n",
    " |   **x** |  837 | 1  |\n",
    " |   **y**| 1  |  66 |\n",
    "\n",
    "\n",
    "**Covariance matrix III:**\n",
    "\n",
    "| C3 | x | y |\n",
    "| ---|---|---|\n",
    "|   **x** |  837 | 453  |\n",
    "|   **y**| 453  |  318 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf45462c85c6d9081616a70c6b5cac34",
     "grade": true,
     "grade_id": "cell-cedc2d0ce28f6b8d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Dataset $c$ corresponds to matrix $II$, because of the low $y$ variance and the almost non-existent covariance between $x$ and $y$.  \n",
    "The $x$ variance is the same for all matrices, but the $y$ variance is very high for $I$ and a lot lower for $III$, which indicates\n",
    "that dataset $b$ corresponds to matrix $I$ and therefore $a$ to matrix $III$. The higher covariance between $x$ and $y$ in matrix $I$ also confirms that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4aeceb0554feb56e12a287d23bdb1e17",
     "grade": false,
     "grade_id": "cell-a93ee44940ef1b55",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** Implement the formula for the autocorrelation matrix\n",
    "\n",
    "$$ A = \\frac{1}{|D|} \\sum_{\\overrightarrow{x} \\in D} (\\frac{(\\overrightarrow{x} - \\overrightarrow{\\mu})}{\\overrightarrow{s}})^T \\frac{(\\overrightarrow{x}- \\overrightarrow{\\mu})}{\\overrightarrow{s}} $$\n",
    "\n",
    "Hint: For your implementation, it might be more efficient to divide by the matrix of standard deviations in the end. The matrix of standard deviations for two dimensions looks like this:\n",
    "\n",
    "| S | x | y |\n",
    "| ---|---|---|\n",
    "|   **x** |  $$s_{x}s_{x}$$| $$s_{x}s_{y}$$ |\n",
    "|   **y**| $$s_{y}s_{x}$$ |  $$s_{y}s_{y}$$ |\n",
    "\n",
    "\n",
    "Make sure to make your implementation work for more than two dimensional data as well!\n",
    "\n",
    "Then execute the cell below which shows the autocorrelation matrices for the datasets of task c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "735e3647eea6c6a4f5d1436a8a64b403",
     "grade": true,
     "grade_id": "cell-00c56f79c4638bbdd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def autocorrelation(data):\n",
    "    \"\"\"\n",
    "    Input is an array of the form number_data * dimensions\n",
    "    An example is shown when this cell is executed\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "## DELETE COMMENTS TO SHOW THE DATA USED FOR ASSERTION \n",
    "# example_x = [2,2,2]\n",
    "# example_y = [1,2,3]\n",
    "# example_data = np.stack([[1,2,3], [2,2,2]], axis=1)\n",
    "# plt.scatter(example_x, example_y)\n",
    "# plt.show()\n",
    "\n",
    "test_data1 = np.stack([[1,2,3], [2,2,2]], axis=1)\n",
    "cor1 = autocorrelation(test_data1)\n",
    "real_cor1 = [[1.0, 0.0], [0.0, 0.0]]\n",
    "\n",
    "test_data2 = np.stack([[1,2,3], [4,5,6]], axis=1)\n",
    "cor2 = autocorrelation(test_data2)\n",
    "real_cor2 = [[1,1], [1,1]]\n",
    "\n",
    "#assert np.allclose(cor1, real_cor1), \"Wrong output: correlation should be \\n {} \\n for this data, is \\n {}\".format(real_cor1, cor1)\n",
    "assert np.allclose(cor2, real_cor2), \"Wrong output: correlation should be \\n {} \\n for this data, is \\n {}\".format(real_cor2, cor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data of task c is saved in a list called datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    print(\"Autocorrelation matrix for data set\", names[i], \"\\n\", autocorrelation(d), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed87dd3d1cd07f2cc36f6ed09074fd42",
     "grade": false,
     "grade_id": "cell-6dc44b01aa2a6ebb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**e)** What is the difference between the autocorrelation matrix and covariance matrix? When should it be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af194a450ceab79586d38356d0763c32",
     "grade": true,
     "grade_id": "cell-77e57257880bd198",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a382cc7af13dc12bc9d930259a03470e",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Local PCA (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7d73e08c8691a284a111ef4e5754489",
     "grade": false,
     "grade_id": "ex_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the lecture we learned that regular PCA is ill suited for special cases of data. In this assignment we will take a look at local PCA which is used for clustered data (ML-06, Slide 25). This is mostly a repetition of algorithms we already used. Feel free to use the built-in functions for k-means clustering and PCA from the libraries (we already included the right imports to set you on track)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fbdfdf2d5590511defbd5a98f30fc2f",
     "grade": true,
     "grade_id": "ex_a_solution",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import multivariate_normal as multNorm\n",
    "\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate clustered data - you may plot the data to take a look at it\n",
    "data = np.vstack((multNorm([2, 2], [[0.1, 0], [0, 1]], 100),\n",
    "                  multNorm([-2, -4], [[1, 0], [0, 0.3]], 100)))\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(data[:, 0],data[:, 1])\n",
    "plt.title(\"Dataset\", fontsize = 15)\n",
    "plt.xlabel(\"x\", fontsize = 15)\n",
    "plt.ylabel(\"y\", fontsize = 15)\n",
    "plt.ylim(-7, 7)\n",
    "plt.xlim(-7, 7)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Apply k-means to the data.\n",
    "# YOUR CODE HERE\n",
    "# compute centroids using k-means\n",
    "centroids, distortion = kmeans(data, 2)\n",
    "# assign labels to data\n",
    "labels, dist = vq(data, centroids)\n",
    "\n",
    "# TODO: Apply PCA for each cluster and store each two largest components.\n",
    "# YOUR CODE HERE\n",
    "unique_labels = np.unique(labels)\n",
    "pcs = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    pca = PCA(n_components=2)\n",
    "    # perform PCA for cluster\n",
    "    cluster_data = [data[i] for i in range(len(data)) if labels[i] == label]\n",
    "    pca.fit(cluster_data)\n",
    "    pcs[label] = pca\n",
    "\n",
    "# TODO: Plot the results of k-means and local PCA\n",
    "# YOUR CODE HERE\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = plt.gca()\n",
    "    arrowprops = dict(arrowstyle='->', linewidth=3, shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "plt.title(\"Local PCA\", fontsize=15)\n",
    "plt.xlabel(\"x\", fontsize=15)\n",
    "plt.ylabel(\"y\", fontsize=15)\n",
    "plt.plot(centroids[:, 0], centroids[:, 1], '.r', markersize=15)\n",
    "\n",
    "for label in unique_labels:\n",
    "    cluster_data = np.array([data[i] for i in range(len(data)) if labels[i] == label])\n",
    "    plt.scatter(cluster_data[:, 0], cluster_data[:, 1])\n",
    "    pca = pcs[label]\n",
    "    for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "        v = vector * 3 * np.sqrt(length)\n",
    "        draw_vector(pca.mean_, pca.mean_ + v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6997ee0313f994aec208454f450bb1b",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Data Visualization and Chernoff Faces (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4828f2fd814bc44d0764ad0673577296",
     "grade": false,
     "grade_id": "ex2_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following exercise contains no programming (unless you want to go through the implementation). Answer the questions that are posted below the code segment (and run the code before - it's really worth it!). In case you are even more interested - here is a link to the [original paper](http://www.dtic.mil/cgi-bin/GetTRDoc?AD=AD0738473)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse, Arc\n",
    "from numpy.random import rand\n",
    "import numpy as np\n",
    "\n",
    "def cface(ax, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18):\n",
    "    \"\"\"\n",
    "    This implementation of chernov faces is taken from Abraham Flaxman. You can\n",
    "    find the original source files here: https://gist.github.com/aflaxman/4043086\n",
    "    Only minor adjustments have been made.\n",
    "\n",
    "     x1 = height  of upper face\n",
    "     x2 = overlap of lower face\n",
    "     x3 = half of vertical size of face\n",
    "     x4 = width of upper face\n",
    "     x5 = width of lower face\n",
    "     x6 = length of nose\n",
    "     x7 = vertical position of mouth\n",
    "     x8 = curvature of mouth\n",
    "     x9 = width of mouth\n",
    "     x10 = vertical position of eyes\n",
    "     x11 = separation of eyes\n",
    "     x12 = slant of eyes\n",
    "     x13 = eccentricity of eyes\n",
    "     x14 = size of eyes\n",
    "     x15 = position of pupils\n",
    "     x16 = vertical position of eyebrows\n",
    "     x17 = slant of eyebrows\n",
    "     x18 = size of eyebrows\n",
    "    \"\"\"\n",
    "\n",
    "    # transform some values so that input between 0,1 yields variety of output\n",
    "    x3 = 1.9 * (x3 - .5)\n",
    "    x4 = (x4 + .25)\n",
    "    x5 = (x5 + .2)\n",
    "    x6 = .3 * (x6 + .01)\n",
    "    x8 = 5 * (x8 + .001)\n",
    "    x11 /= 5\n",
    "    x12 = 2 * (x12 - .5)\n",
    "    x13 += .05\n",
    "    x14 += .1\n",
    "    x15 = .5 * (x15 - .5)\n",
    "    x16 = .25 * x16\n",
    "    x17 = .5 * (x17 - .5)\n",
    "    x18 = .5 * (x18 + .1)\n",
    "\n",
    "    # top of face, in box with l=-x4, r=x4, t=x1, b=x3\n",
    "    e = Ellipse((0, (x1 + x3) / 2), 2 * x4, (x1 - x3), ec='black', linewidth=2)\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    # bottom of face, in box with l=-x5, r=x5, b=-x1, t=x2+x3\n",
    "    e = Ellipse((0, (-x1 + x2 + x3) / 2), 2 * x5, (x1 + x2 + x3), fc='white', ec='black', linewidth=2)\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    # cover overlaps\n",
    "    e = Ellipse((0, (x1 + x3) / 2), 2 * x4, (x1 - x3), fc='white', ec='none')\n",
    "    ax.add_artist(e)\n",
    "    e = Ellipse((0, (-x1 + x2 + x3) / 2), 2 * x5, (x1 + x2 + x3), fc='white', ec='none')\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    # draw nose\n",
    "    plt.plot([0, 0], [-x6 / 2, x6 / 2], 'k')\n",
    "\n",
    "    # draw mouth\n",
    "    p = Arc((0, -x7 + .5 / x8), 1 / x8, 1 / x8, theta1=270 - 180 / np.pi * np.arctan(x8 * x9),\n",
    "            theta2=270 + 180 / np.pi * np.arctan(x8 * x9))\n",
    "    ax.add_artist(p)\n",
    "\n",
    "    # draw eyes\n",
    "    p = Ellipse((-x11 - x14 / 2, x10), x14, x13 * x14, angle=-180 / np.pi * x12, fc='white', ec='black')\n",
    "    ax.add_artist(p)\n",
    "\n",
    "    p = Ellipse((x11 + x14 / 2, x10), x14, x13 * x14, angle=180 / np.pi * x12, fc='white', ec='black')\n",
    "    ax.add_artist(p)\n",
    "\n",
    "    # draw pupils\n",
    "    p = Ellipse((-x11 - x14 / 2 - x15 * x14 / 2, x10), .05, .05, facecolor='black')\n",
    "    ax.add_artist(p)\n",
    "    p = Ellipse((x11 + x14 / 2 - x15 * x14 / 2, x10), .05, .05, facecolor='black')\n",
    "    ax.add_artist(p)\n",
    "\n",
    "    # draw eyebrows\n",
    "    ax.plot([-x11 - x14 / 2 - x14 * x18 / 2, -x11 - x14 / 2 + x14 * x18 / 2],\n",
    "            [x10 + x13 * x14 * (x16 + x17), x10 + x13 * x14 * (x16 - x17)], 'k')\n",
    "    ax.plot([x11 + x14 / 2 + x14 * x18 / 2, x11 + x14 / 2 - x14 * x18 / 2],\n",
    "            [x10 + x13 * x14 * (x16 + x17), x10 + x13 * x14 * (x16 - x17)], 'k')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, num='Chernoff Faces', figsize=(9, 9))\n",
    "for ax in axes.flat:\n",
    "    cface(ax, .9, *rand(17))\n",
    "    ax.axis([-1.2, 1.2, -1.2, 1.2])\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f156ca42cad42d49ededbf47abf8dd9",
     "grade": false,
     "grade_id": "ex2_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) Data Visualization Techniques\n",
    "\n",
    "Why do we need data visualization techniques and what are techniques to visualize high dimensional data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7180ee205e447ec74954fb547aed4fa",
     "grade": true,
     "grade_id": "ex2_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Data visualization techniques make complex data more accessible, understandable and usable for humans.  \n",
    "One technique to visualize high dimensional data would be PCA, it is often able to make hidden structure in the data visible.  \n",
    "Howevery, PCA is not always able to visualize the data in an appropriate way. We have seen a couple of techniques in the lecture that can help when trying to visualize high dimensional data such as glyphs, scatterplot matrices and other projection techniques besides PCA.  \n",
    "If possible, it's often a good idea to project the data in a lower dimensional space, but this can lead to an information loss which it not always feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea86a6cbb36039ff38544d8db40b61af",
     "grade": false,
     "grade_id": "ex2_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Chernoff faces\n",
    "\n",
    "Why did Chernoff use faces for his representation? Why not something else, like dogs or houses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dab2c90645187b4347545c46a591364b",
     "grade": true,
     "grade_id": "ex2_b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The human visual system is particularly sensitive for faces, i.e. we are very good at interpreting and recognizing human faces.  \n",
    "Chernoff used faces to make use of this property when representing high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e541a1ef74e6a8bc43b1029e4bfe3129",
     "grade": false,
     "grade_id": "ex2_c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### c) Alternatives\n",
    "\n",
    "Explain at least one other data visualization technique from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66556e394184be3dfd3b9dbeac95c483",
     "grade": true,
     "grade_id": "ex2_c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "- **Scatterplot matrix**: Project on two of the dimensions and display all combinations as matrix of 2D plots\n",
    "- **Glyphs**: Map each dimension onto the parameters of a geometrical figure\n",
    "- **Projection techniques**: Do not represent the complete information, but project onto selected directions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda475831754d4f421c8b9b78a005f24ea9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}