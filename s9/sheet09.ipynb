{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb881b440faff8e8427d5a2861f39e9f",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "OsnabrÃ¼ck University - Machine Learning (Summer Term 2020) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0524f468c9382b8f98fc2b672ff4d290",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d84f39aac4a356d9d7a4f5a1298cd24",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Saturday, July 04, 2020**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33ba5f97b56822d4c88b98edec4740b4",
     "grade": false,
     "grade_id": "cell-43f31bf558c5a8ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Again, the second half of this sheet will be a recap of previous topics, to help you prepare for the final exam.\n",
    "\n",
    "Also if you hit any question that should be discussed in more detail in the next practice session, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28c8cdfad20451131397a80a6e4cd6b6",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Reinforcement Learning [12 Points]\n",
    "\n",
    "In this assignment you will have a look at the Q-Learning algorithm described in the lecture (ML-10 Slide 18). For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below the code there are some questions! You also find a free-code field for a complete own implementation. You may use your own test mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x (int):            x dimension of the field\n",
    "        y (int):            y dimension of the field \n",
    "        num_rewards (int):  the number of rewards that should be randomly placed\n",
    "        max_reward (int):   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        ndarray: A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change or comment out to get different random data in each run\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    field = np.zeros((y,x), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0347ab3dd299f99d9337f67b0ab86f50",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, actions, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            field (ndarray):  the map\n",
    "            actions (list):   the available actions\n",
    "            gamma (float):    the gamma in the lecture slides\n",
    "        \n",
    "        Returns:\n",
    "            QLearning: An instance that can be used for Q-Learning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field.\n",
    "        self.field = field\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Remember the map extend for further navigation.\n",
    "        self.map_y = self.field.shape[0]\n",
    "        self.map_x = self.field.shape[1]\n",
    "        \n",
    "        # Create q value matrix.\n",
    "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
    "\n",
    "        # Start on a random position in the field.\n",
    "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
    "        self.fig, self.axes = plt.subplots(3, 3, num='QLearning State')\n",
    "        for ax in self.axes.flat:\n",
    "            ax.axis('off')\n",
    "\n",
    "    def get_coordinates(self, position, action):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain action, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            position (pair):  the current position\n",
    "            action (string):  the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            pair of int: the updated coordinates\n",
    "        \"\"\"\n",
    "        # return the right new coordinates depending on the position\n",
    "        # YOUR CODE HERE\n",
    "        x = position[1]\n",
    "        y = position[0]\n",
    "        \n",
    "        if action == 'left':\n",
    "            return (y, x - 1) if x > 0 else (y, x)\n",
    "        elif action == 'right':\n",
    "            return (y, x + 1) if x < self.map_x - 1 else (y, x)\n",
    "        elif action == 'up':\n",
    "            return (y - 1, x) if y > 0 else (y, x)\n",
    "        elif action == 'down':\n",
    "            return (y + 1, x) if y < self.map_y - 1 else (y, x)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # Select a random action that should be performed next.\n",
    "        # Be careful to handle the case where you hit the wall!\n",
    "        # YOUR CODE HERE\n",
    "        rand_action = np.random.choice(self.actions)\n",
    "        resulting_pos = self.get_coordinates(self.pos, rand_action)\n",
    "\n",
    "        if resulting_pos != self.pos:\n",
    "            # Receive the reward for the new position from the field.\n",
    "            # YOUR CODE HERE\n",
    "            reward = self.field[resulting_pos[0], resulting_pos[1]]\n",
    "            \n",
    "            # Update the q-value for the performed action.\n",
    "            # YOUR CODE HERE\n",
    "            q_values = self.q[:, resulting_pos[0], resulting_pos[1]]\n",
    "            q_update = reward + self.gamma * max(q_values)\n",
    "            # we have one table for each action\n",
    "            self.q[self.actions.index(rand_action), self.pos[0], self.pos[1]] = q_update\n",
    "\n",
    "            # Update the position of the player to the new field.\n",
    "            # YOUR CODE HERE\n",
    "            self.pos = resulting_pos\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current state.\n",
    "        \"\"\"\n",
    "        fs = 8\n",
    "        for i, action in enumerate(self.actions):\n",
    "            ax = self.axes.flat[2*i + 1]\n",
    "            ax.cla()\n",
    "            ax.set(title=action)\n",
    "            ax.set_xticks(np.arange(self.q[i,:,:].shape[1]))\n",
    "            ax.set_yticks(np.arange(self.q[i,:,:].shape[0]))\n",
    "            ax.imshow(self.q[i,:,:], interpolation='None')\n",
    "\n",
    "            for j in range(self.q.shape[1]):\n",
    "                for k in range(self.q.shape[2]):\n",
    "                    text = ax.text(k, j, \"{:.1f}\".format(self.q[i,j,k],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "                    plt.setp(text, path_effects=[\n",
    "        PathEffects.withStroke(linewidth=1, foreground=\"w\")])\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc6dabd4d8485aac17168e0d2cefa8a",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 5\n",
    "m_y = 4\n",
    "\n",
    "steps = 500\n",
    "\n",
    "actions = ['up','left','right','down']  # Those are the availabe actions for the QLearning.\n",
    "field = generate_field(m_x, m_y, num_rewards=5, max_reward=10) # The field that is used for learning.\n",
    "\n",
    "# Plotting the generated field\n",
    "fs = 18\n",
    "figure, ax = plt.subplots()\n",
    "#plt.axis('off')\n",
    "ax.imshow(field, interpolation='none')\n",
    "ax.set_xticks(np.arange(field.shape[1]))\n",
    "ax.set_yticks(np.arange(field.shape[0]))\n",
    "for j in range(field.shape[0]):\n",
    "    for k in range(field.shape[1]):\n",
    "        text = plt.text(k, j, field[j,k],\n",
    "        ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "        plt.setp(text, path_effects=[PathEffects.withStroke(linewidth=3, foreground=\"w\")])\n",
    "\n",
    "figure.suptitle(\"Field\",fontsize=fs)          \n",
    "figure.canvas.draw()\n",
    "\n",
    "\n",
    "# Generate a QLearning instance with the right parameters.\n",
    "# YOUR CODE HERE\n",
    "player = QLearning(field, actions, 0.85)\n",
    "\n",
    "# Now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance.\n",
    "for i in range(steps):     \n",
    "    player.update()\n",
    "    player.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abb7d97c008b533c8d2649cd3e54f3c6",
     "grade": false,
     "grade_id": "ex2_x",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Explain in your own words, how the algorithm works. What is depicted on the resulting plots. How can an action policy be derived from these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b61f070ffb0153e5dd4767692d90b83",
     "grade": true,
     "grade_id": "ex2c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Q-learning is a RL technique used for learning the optimal policy in a MDP (in the sense of maximizing the expected value of the total reward over all possible steps, starting from the current state). \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state.\n",
    "\n",
    "The algorithm starts by initializing all $q$-values to $0$. In the loop, we execute an action and update the $q$-value for the current state and the performed action based on the reward of performing this action in the current state and the maximum $q$-value over all possible actions in the resulting state weighted by the discount factor. Afterwards, the current state gets updated to the new one.\n",
    "\n",
    "The field plot just shows the reward for ending up in each field.  \n",
    "The plots below the field plot show the $q$-values for each of the possible actions. A cell in one of the tables contains the $q$-value for performing that specific action when being on that position on the field. For example, when the current position on the field is $(1, 4)$, it's a good idea to move up and reach the state with a high reward of $7$, that's why 'up' has a high $q$-value at this position.\n",
    "\n",
    "The plots always show a light color for cells with a high reward or $q$-value and a dark one for low values.\n",
    "\n",
    "An action policy can be derived by suggesting the action with the highest $q$-value on the specified field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf1427a0356c02f45ce383edafe01b5",
     "grade": false,
     "grade_id": "ex2_0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm (instead of completing the code above). Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b5b89fbaa3fca39dd0be4bebb6c66fd",
     "grade": false,
     "grade_id": "recap2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap (part II)\n",
    "\n",
    "This is the second part of the recap material. These exercises do not need to be solved in order to qualify for the final exam but it is highly recommended for preparation. Also if you hit any question that should be discussed in more detail, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8dbc3ef48a5ab1fc8833902a5750697",
     "grade": false,
     "grade_id": "r06",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 6: Neural Networks [3 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3de6953a83ac01bf99e492bca3f62693",
     "grade": false,
     "grade_id": "r06a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Neural Networks\n",
    "\n",
    "Name three different kinds of Artificial Neural Networks discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc1b84582772025c71299094937c3bfa",
     "grade": true,
     "grade_id": "r06a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Multilayer perceptron (MLP)**\n",
    "- **Radial basis function networks (RBFN)**\n",
    "- **Self-organizing maps (SOM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a66ab87c34b88f7d75630775c1acb98",
     "grade": false,
     "grade_id": "r06b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Backpropagation\n",
    "\n",
    "Which of the following formulae describes the backpropagation of the error through hidden layers in a Multilayer Perceptron?\n",
    "Assume they are calculated for each $k=L_H \\dots 1$ and $i=1\\dots N(k)$.\n",
    "\n",
    "1. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1, k)o_j(k)$\n",
    "2. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1, k)\\delta_j(k+1)$\n",
    "3. $\\delta_i(k) = f^\\prime(o_i(k)) \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k, k-1)\\delta_j(k+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8d8027f1b1638db0833711498dac467",
     "grade": true,
     "grade_id": "r06b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Formula $2$ is the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b2383196240f8015fce8111c44618c7",
     "grade": false,
     "grade_id": "r06c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) Hebb's rule\n",
    "Explain Hebb's rule. Provide a formula. What is the relation to Oja's rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "664e40dd0aed7acfe1360fc06d9464d9",
     "grade": true,
     "grade_id": "r06c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea of Hebb's rule can be summarized by the catch phrase 'neurons that fire together, wire together'.  \n",
    "It can be expressed by the formula: $\\Delta w_i = \\epsilon \\cdot y(\\vec{x}\\vec{w})x_i$  \n",
    "Hebb's rule has the problem that weights never decrease, they become arbitrarily large.  \n",
    "Oja's rule is an approach to deal with that issue by introducing a weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af1564b6767d050589c7ff767121c17a",
     "grade": false,
     "grade_id": "r07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 7: Local Methods [2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f90b47c72735d7628fb7ec7cf3bc623",
     "grade": false,
     "grade_id": "r07a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Local methods\n",
    "\n",
    "What are differences between local and global methods? What are advantages or disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b04430591b527b08dcf411dc922e1d7",
     "grade": true,
     "grade_id": "r07a_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Local methods are local with respect to the input space: The output is computed individually for different regions of the input space, so adaptation has only local effects. MLPs for example are not local, adaptation of a single weight based on a single example may influence the performance of the entire net (all output channels) and on the complete set of inputs.  \n",
    "\n",
    "Advantages of local methods:\n",
    "- better in dealing with noise in the training data\n",
    "- better suited to deal with data flows that are dynamically evolving\n",
    "- effects of parameters are easier to interpret\n",
    "\n",
    "**Question:** What are disadvantages of local methods?  \n",
    "Maybe in some scenarios it would be desirable to have the whole net adapt very fast according to dynamic changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69286c3f3433fc504bba0b3a6789b322",
     "grade": false,
     "grade_id": "r07b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) MLP and RBFN\n",
    "\n",
    "Is an MLP or are RBFN local methods? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caacbd38edeb60e5cb2fd5fa56098c01",
     "grade": true,
     "grade_id": "r07b_solution",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A MLP is a non-local method because the adaptation of a single weight based on a single example may influence the performance of the entire net. RBFN on the other hand are local methods since each neuron only affects its own receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1415431834f79d0e1243bbafb7570b16",
     "grade": false,
     "grade_id": "r07c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c)  Nearest neighbor\n",
    "\n",
    "How does the nearest neighbor approach work? How can it be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f85205012c26449e35a517cdf9f9918",
     "grade": true,
     "grade_id": "r07c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **nearest neighbor** approach first memorizes all examples ('training'). In the application phase, the output of the best match for the input vector $\\vec{x}$ from the memorized examples is used as output. An extension for this approach is the **$k$-nearest neighbor** approach where not only one nearest neighbor from the set of memorized examples is considered, but $k$. For discrete valued outputs, one votes among the $k$ nearest neighbors to select one and for the continuous case, one simply takes the mean. An even more sophisticated approach would be to use **distance-weighted $k$-nearest neigbors** where k-nearest neighbors is improved by weighting with the distance to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17e234d68888e1f5d2302c84f2b28372",
     "grade": false,
     "grade_id": "r08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap 8: Classification [3 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2abe8e72e4c19c2e40830204a9d8a688",
     "grade": false,
     "grade_id": "r08a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### a) Classfier\n",
    "\n",
    "What is a classifier? What is the relation to a concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0f7fd2628c117465542d91084f03995",
     "grade": true,
     "grade_id": "r08a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A **classifier** assigns a discrete class to an object based on attributes:  \n",
    "e.g. assign class *dry* to a *towel*\n",
    "\n",
    "A **concept** can be represented by a boolean function which assigns true to the appropriate entities:  \n",
    "e.g. car(thisChair) = $false$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5b630f3827ca3bf0d814cb58281196e",
     "grade": false,
     "grade_id": "r08b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### b) Comparison of classifiers\n",
    "\n",
    "Name three different classifiers and compare them. Think about biases and assumptions, separatrices, sensitivity, locality, parameters and speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3b9d5888fdc6d49034f6ebb93afb14a",
     "grade": true,
     "grade_id": "r08b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| classifier | biases (assumptions) | separatrices | sensitivity | locality | parameters | speed |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Euclidean classifier | **no idea** | linear | sensitive to far outliers | not local | - | very fast |\n",
    "| Quadratic classifier | **no idea** | conic section | **no idea** | not local | - | fast |\n",
    "| Nearest neighbor classifier | nbrs should have same / similar classification | implicitly defined by nbrs | **no idea** | local | - | depends on size |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa0cbd27cac42fa8e37044709288123f",
     "grade": false,
     "grade_id": "ex08c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### c) SVM\n",
    "\n",
    "What is a support vector? How does the kernel trick work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f675e71da82dca8138ab125d6ac4309",
     "grade": true,
     "grade_id": "r08c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The support vectors are examples close to the class boundary.\n",
    "\n",
    "The kernel trick is used to solve non-linear problems:\n",
    "- the data is projected into a higher dimensional space\n",
    "- for sufficiently high dimension, every problem becomes linearly separable by a hyperplane\n",
    "- the projection of this hyperplane back to the original data space is a non-linear separatrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda475831754d4f421c8b9b78a005f24ea9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}