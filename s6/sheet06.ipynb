{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb881b440faff8e8427d5a2861f39e9f",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2020) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "373ff226c4bac745ccd22953579ba1bf",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d83f2cecec38a4df6b05df61ceafa549",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Saturday, June 13, 2020**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b529b62ef26733edb117e7ea309b1c27",
     "grade": false,
     "grade_id": "math-matrix",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Hyperplanes) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc371cf3fc1bd7ae06f900316e70f3f6",
     "grade": false,
     "grade_id": "math-hyperplane-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *hyperplane*? What are the hyperlanes in $\\mathbb{R}^2$ and $\\mathbb{R}^3$? How are the usually described?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88a263200b0663825b9c7186013a0472",
     "grade": true,
     "grade_id": "math-hyperplane-a1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afd504b56e1d9fc5d34933d799a2b15b",
     "grade": false,
     "grade_id": "math-hyperplane-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the Hesse normal form? What is the intuition behind? What are its advantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "021e464996320a7a79691045039b51fd",
     "grade": true,
     "grade_id": "math-hyperplane-a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ed69a2bc2a9faea213969bcec87db1b",
     "grade": false,
     "grade_id": "math-hyperplane-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Can you transform the standard form of a hyperplane into the Hesse normal form and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "393bcc89d193ac13efb934c0dea177ae",
     "grade": true,
     "grade_id": "math-hyperplane-a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5abbe7babf270e6ef8cab923e19933ad",
     "grade": false,
     "grade_id": "ex_hebb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Hebbian Learning (6 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "721c23b72b7577a4b0a634d4a478f4b4",
     "grade": false,
     "grade_id": "ex_hebb_info",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the lecture (ML-07, Slides 10ff.) there is a simplified version of Ivan Pavlov's famous experiment on classical conditioning. In this exercise you will take a look into this simplified model and create your own conditionable dog with a simple Hebbian learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8dcbf1cc2b9208dc5f4fb7994c6ca60",
     "grade": false,
     "grade_id": "ex_hebb_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) Programming a Dog\n",
    "To model the dog salivation behavior we will need to model an unconditioned and a conditioned stimulus: food and bell. They are represented as lists: `weight_food` and `weight_bell`. Note that one could just use a single number, the lists are only here to keep track of the history for a nice output. It is possible to access the current weight by selecting the last item of each list, respectively: `weight_food[-1]`.\n",
    "\n",
    "A list of trials is already given as well as a condition database. Each entry represents an index to select from the `condition_db`. To figure out the value of the stimulus `food` in the second trial (which maps to condition `1`) one could do: `condition_db[1][\"food\"]`.\n",
    "\n",
    "Your task is to implement a `for` loop over all trials. In each iteration select the correct values for $x_1$ and $x_2$ from the condition database and retrieve the current weights $w_1$ and $w_2$. Then calculate the response of the dog with the threshold $\\theta$:\n",
    "\n",
    "$$\n",
    "r_t = \\Theta(x_{1,t} w_{1,t-1} + x_{2,t} w_{2,t-1})\\\\\n",
    "\\Theta(x)= \\begin{cases}1 \\text{ if } x >= \\theta\\\\0 \\text{ else }\\end{cases}\n",
    "$$\n",
    "\n",
    "With this response calculate both $w_{n,t}$ according to the Hebbian rule:\n",
    "\n",
    "$$w_{n,t} = w_{n, t-1} + \\epsilon \\cdot r_t \\cdot x_{n,t}$$\n",
    "\n",
    "*Note: While you program the output might look a little messy, don't worry about it. Once you fill up all three lists properly, it will look much like on ML-07, Slide 14.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f591e4583e457d590a714d24fdac0ef",
     "grade": true,
     "grade_id": "ex_hebb_a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "| Food   |   |  1|   |  0|   |  1|   |  0|   |  1|   |  0|   |  1|   |  0|   |\n| Bell   |   |  0|   |  1|   |  1|   |  1|   |  1|   |  1|   |  1|   |  1|   |\n| Saliva |   |  1|   |  0|   |  1|   |  0|   |  1|   |  0|   |  1|   |  1|   |\n| w_Food |1.0|   |1.2|   |1.2|   |1.4|   |1.4|   |1.6|   |1.6|   |1.8|   |1.8|\n| w_Bell |0.0|   |0.0|   |0.0|   |0.2|   |0.2|   |0.4|   |0.4|   |0.6|   |0.8|\n"
    }
   ],
   "source": [
    "# Initialization\n",
    "condition_db = [{\"food\": 1, \"bell\": 0}, \n",
    "                {\"food\": 0, \"bell\": 1},\n",
    "                {\"food\": 1, \"bell\": 1}]\n",
    "\n",
    "trials = [0, 1, 2, 1, 2, 1, 2, 1]\n",
    "\n",
    "epsilon = 0.2\n",
    "theta = 1/2\n",
    "\n",
    "responses = []\n",
    "weight_food = [1]\n",
    "weight_bell = [0]\n",
    "\n",
    "# TODO: For each trial, update the current weights of the US and CS and store\n",
    "# the results in the respective lists. Also store the response.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "for t in trials:\n",
    "    x_1 = condition_db[t][\"food\"]\n",
    "    x_2 = condition_db[t][\"bell\"]\n",
    "    w_1 = weight_food[-1]\n",
    "    w_2 = weight_bell[-1]\n",
    "\n",
    "    resp = x_1 * w_1 + x_2 * w_2 >= theta\n",
    "    responses.append(resp)\n",
    "    weight_food.append(w_1 + epsilon * resp * x_1)\n",
    "    weight_bell.append(w_2 + epsilon * resp * x_2)\n",
    "\n",
    "# Output\n",
    "print(\"| Food   |   |\" + \"|   |\".join([\"{:3d}\".format(condition_db[trial][\"food\"]) for trial in trials]) + \"|   |\")\n",
    "print(\"| Bell   |   |\" + \"|   |\".join([\"{:3d}\".format(condition_db[trial][\"bell\"]) for trial in trials]) + \"|   |\")\n",
    "print(\"| Saliva |   |\" + \"|   |\".join([\"{:3d}\".format(response) for response in responses]) + \"|   |\")\n",
    "print(\"| w_Food |\" + \"|   |\".join([\"{:3.1f}\".format(w) for w in weight_food]) + \"|\")\n",
    "print(\"| w_Bell |\" + \"|   |\".join([\"{:3.1f}\".format(w) for w in weight_bell]) + \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7827fb0b6078f375e088294ee5d49c",
     "grade": false,
     "grade_id": "ex_hebb_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Parameter adjustment\n",
    "\n",
    "In the above default setting of trials (`[0, 1, 2, 1, 2, 1, 2, 1]`, in case you changed it), how many learning steps did you need until the dog started to produce saliva on the conditioned stimulus? What happens if you change the parameters $\\epsilon$ and $\\theta$? Try smaller and bigger values for each or present different conditions to the dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3ad750be3516565704a30157191e919",
     "grade": true,
     "grade_id": "ex_hebb_b_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Since the learning takes place when both stimuli are present, we need $3$ learning steps in the above setting.  \n",
    "If we increase the learning rate $\\epsilon$, we need fewer trials to learn the CS and if we decrease it, the number of necessary learning steps increases. With the threshold $\\theta$ it's the other way around. If $\\epsilon$ would be negative, the dog would unlearn the US (i.e. no salivation for presentation of food). If $\\theta \\geq 1$ there would be no learning as well, because the dog won't salivate for presentation of food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9829060a95442954b2a28ff79bff91f7",
     "grade": false,
     "grade_id": "ex_logic_perceptron",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: The Logic Perceptron [3 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c61ad8c15706e9da888dd11a5829bda",
     "grade": false,
     "grade_id": "ex_logic_perceptron_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) The Logic Perceptron\n",
    "\n",
    "For the following two logical functions sketch the weights of a perceptron after it was trained. To do so, figure out when the perceptron should fire. Then come up with ideas of how you can achieve this. Remember that $w_0$, the bias, is used as a threshold and that there is a constant $x_0 = 1$. Provide the values for $w_0,w_1,w_2$ as well as some explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f48beaf7b1ebc2cb3f49f71fbaec8bb",
     "grade": false,
     "grade_id": "ex_logic_perceptron_a1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1) $(A \\wedge B) \\vee (\\neg A \\wedge B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "857f16cab97fa5bc6675f677d6684628",
     "grade": true,
     "grade_id": "ex_logic_perceptron_a1_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$x_1 := A, \\quad x_2 := B$  \n",
    "\n",
    "The expression only evaluates to $true$ if $B = true$. Therefore the perceptron should fire if $x_2 = 1$.  \n",
    "Perceptron activation:  \n",
    "$s = w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2$  \n",
    "It fires if $s \\geq 0$  \n",
    "\n",
    "Possible combinations:\n",
    "- $x_1 = 0, x_2 = 0: \\quad w_0 + 0 w_1 + 0 w_2 = w_0$\n",
    "- $x_1 = 1, x_2 = 0: \\quad w_0 + 1 w_1 + 0 w_2 = w_0 + w_1$\n",
    "- $x_1 = 0, x_2 = 1: \\quad w_0 + 0 w_1 + 1 w_2 = w_0 + w_2$\n",
    "- $x_1 = 1, x_2 = 1: \\quad w_0 + 1 w_1 + 1 w_2 = w_0 + w_1 + w_2$\n",
    "\n",
    "To achieve the expected behavior, we could choose $w_0 = -1, w_1 = 0.5, w_2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80657938a3785aae4b00c14382108840",
     "grade": false,
     "grade_id": "ex_logic_perceptron_a2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2) $(A \\wedge B) \\vee (\\neg A \\wedge B) \\vee (A \\wedge \\neg B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3dd484ee01532307942f2b0a10c1d41",
     "grade": true,
     "grade_id": "ex_logic_perceptron_a2_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$x_1 := A, \\quad x_2 := B$ \n",
    "\n",
    "The expression only evaluates to $false$ if both $A$ and $B$ are $false$. Therefore the perceptron should fire if $A$ or $B$ is $true$.  \n",
    "\n",
    "Perceptron activation:  \n",
    "$s = w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2$  \n",
    "It fires if $s \\geq 0$  \n",
    "\n",
    "Possible combinations:\n",
    "- $x_1 = 0, x_2 = 0: \\quad w_0 + 0 w_1 + 0 w_2 = w_0$\n",
    "- $x_1 = 1, x_2 = 0: \\quad w_0 + 1 w_1 + 0 w_2 = w_0 + w_1$\n",
    "- $x_1 = 0, x_2 = 1: \\quad w_0 + 0 w_1 + 1 w_2 = w_0 + w_2$\n",
    "- $x_1 = 1, x_2 = 1: \\quad w_0 + 1 w_1 + 1 w_2 = w_0 + w_1 + w_2$  \n",
    "\n",
    "To achieve the expected behavior, we could choose $w_0 = -1, w_1 = 1, w_2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36679386b5f47417637777eaed03e37d",
     "grade": false,
     "grade_id": "ex_perceptron",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Perceptron [7 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc5de55f08182b38ce86c9500644e516",
     "grade": false,
     "grade_id": "ex_perceptron_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise you will implement a simple perceptron as described in the lecture [ML-07 Slide 31]. As with  previous exercises it is possible to not use our premade code blocks but write the single Perceptron completely from scratch (an empty cell to do so can be found [below](#Own-Implementation)). \n",
    "\n",
    "Use the following output function:\n",
    "$$y = \\begin{cases}1 \\quad \\text{if} \\ s > 0\\\\0 \\quad \\text{else}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfad3f16551b25b63432225ffe5e46cf",
     "grade": false,
     "grade_id": "ex_perceptron_x3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The `TODO`'s in the following code segments guide you through what has to be done.\n",
    "\n",
    "*Hint*: If you have problems with `np.arrays` (which usually have shapes like `(13,)`, thus with one degenerate dimension, either set the shapes manually (`my_np_array.shape = (13, 1)`). Other useful functions might be `np.append` or `np.hstack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc89ce18dfbb66de462686b66e32785",
     "grade": true,
     "grade_id": "ex_perceptron_a",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "# TODO: Write the input activation (called net_input) and the output function (called out_fun).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# TODO: Write a function generate_weights that generates N (= number of dimensions) + 1 (w_0) random weights.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac43c1d488044d0678401226886fcffe",
     "grade": false,
     "grade_id": "ex_perceptron_a_assert",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Testing the perceptron with a concrete example ##\n",
    "####################################################\n",
    "\n",
    "# Dimensions for our test.\n",
    "dims = 12\n",
    "\n",
    "# Input is a row vector. (Shape is (1, 13).)\n",
    "D = np.hstack((1, rnd.rand(dims) - 0.5))\n",
    "\n",
    "# Weights are stored in a vector.\n",
    "W = generate_weights(dims)\n",
    "\n",
    "out = out_fun(net_input(D, W))\n",
    "\n",
    "assert out == 1 or out == 0, \"The output has to be either 1 or 0, but was {}\".format(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa4afc40055da2eaa0d6618a5290a176",
     "grade": false,
     "grade_id": "ex_perceptron_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following `eval_network(t, D, W)` function is used to measure the performance of your perceptron for the upcoming task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_network(t, D, W):\n",
    "    \"\"\"\n",
    "    This function takes the trained weights of a perceptron\n",
    "    and the input data (D) as well as the correct target values (t)\n",
    "    and computes the overall error rate of the perceptron.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    size = max(D.shape)\n",
    "    for i in range(size):\n",
    "        out = out_fun(net_input(D[i], W))\n",
    "        error = error + abs(t[i] - out)\n",
    "    # Normalize the error.\n",
    "    try:\n",
    "        return error.item(0) / size\n",
    "    except AttributeError:\n",
    "        return error / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72a6ab6418ee68c0e2b6493287e47ed0",
     "grade": false,
     "grade_id": "ex_perceptron_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we will use the above defined functions to train the perceptron to one of the following logical functions: OR, NAND or NOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5150a083a1438bef84abceadbde1f1fe",
     "grade": false,
     "grade_id": "ex_perceptron_b_plotting",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function_to_learn(selector, function):\n",
    "    \"\"\"\n",
    "    Functional definitions for the perceptron to learn\n",
    "    Instantiates plots for visualization of the decision boundary\n",
    "    :param selector: selects which function to activate\n",
    "    :return function:\n",
    "    \"\"\"\n",
    "    plot_points = [[0,0],[0,1],[1,0],[1,1]]\n",
    "    plot_colors = []\n",
    "\n",
    "    for point in plot_points:\n",
    "        plot_colors.append(function(point[0], point[1]))\n",
    "    for color, point in enumerate(plot_points):\n",
    "        plt.scatter(*point, s=50, c='b' if plot_colors[color] == 1 else 'r')\n",
    "    print(\"Perceptron will now learn '{}'...\\n\\n\".format(selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b07bb0b6923923fd2266593d70b26fc",
     "grade": true,
     "grade_id": "ex_perceptron_b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################################################\n",
    "## Now we train our perceptron! [ML-07 Slide 33] ##\n",
    "###################################################\n",
    "\n",
    "# TODO: Write the update function (name it 'delta_fun')\n",
    "#       for the weights dependent on epsilon, the target,\n",
    "#       the output and the input vector.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Define suitable parameters for your problem.\n",
    "# Use the following names:\n",
    "#   ϵ: learning rate\n",
    "#   dims: dimensions\n",
    "#   training_size: the number of training samples\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Generate the weights (in a variable called W).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Generate a matrix D of truthvalue pairs.\n",
    "# The shape should be (training_size, dims).\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Pad the input D with ones for the bias. The bias should always be\n",
    "# w_0, i. e. the first column of the data should be ones.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Learn one of the logical functions OR, NAND, NOR\n",
    "# (the lambda keyword is just a short way to define functions).\n",
    "log_operators = {\n",
    "    'and': lambda x1, x2: x1 and x2,\n",
    "    'or': lambda x1, x2: x1 or x2,\n",
    "    'nand': lambda x1, x2: not (x1 and x2),\n",
    "    'nor': lambda x1, x2: not (x1 or x2),\n",
    "    'xor': lambda x1, x2: (x1 and not x2) or (not x1 and x2)\n",
    "}\n",
    "\n",
    "# Change these two lines to choose the other operators:\n",
    "op = 'and'\n",
    "log_operator = log_operators[op]\n",
    "function_to_learn(op, log_operator)\n",
    "\n",
    "row_operator = lambda row: log_operator(row[0], row[1])\n",
    "labels = np.apply_along_axis(row_operator, 1, D[:, 1:])\n",
    "\n",
    "epochs = 200    # Extra question: What effects do changes in the epochs \n",
    "samp_size = 5  #                 and sample sizes have on our training?\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Sample random from the training data.\n",
    "    for idx in rnd.choice(range(training_size), samp_size, replace=False):\n",
    "        y = out_fun(net_input(D[idx], W))\n",
    "        W += delta_fun(ϵ, labels[idx], y, D[idx])\n",
    "    # Plotting code\n",
    "    y_point = (0, (-W[0] / W[2]))\n",
    "    x_point = ((-W[0] / W[1]), 0)\n",
    "    try:\n",
    "        slope = (y_point[1] - x_point[1]) / (y_point[0] - x_point[0]) # will not work if x and y intercepts are 0\n",
    "    except ZeroDivisionError:\n",
    "        print(\"X and Y intercepts are both zero.  Due to the way slope is calculated, this causes a division by zero.  Sorry.\")\n",
    "    y_out = lambda points: slope * points\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    plt.plot(x, y_out(x) + y_point[1], 'g--', linewidth=3, alpha=i/epochs +.2 if i/epochs +.2 < 1 else 1)\n",
    "    \n",
    "plt.ylim([-.2, 1.2])\n",
    "plt.xlim([-.2, 1.2])\n",
    "plt.title(\"Logic Perceptron (Blue=True)\")\n",
    "plt.xlabel(\"True(1) or False(0)\")\n",
    "plt.ylabel(\"True(1) or False(0)\")\n",
    "plt.show()\n",
    "\n",
    "# Print the overall performance of the Perceptron.\n",
    "print(\"Overall error of the Perceptron: {:.2%}\".format(eval_network(labels, D, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41c3950f547c0c8ad5fcb79d853afad3",
     "grade": false,
     "grade_id": "ex_perceptron_x4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Own Implementation\n",
    "\n",
    "Skip this if you already implemented the perceptron above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fbb2c7f4f5962bbfdf4ebff78c8770f",
     "grade": true,
     "grade_id": "ex_perceptron_x4_solution",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Space for complete own implementation\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "767cd98a8f8f2c7c577666ea0bc82da8",
     "grade": false,
     "grade_id": "ex_sigmoid",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 4: Sigmoid Activation & Backpropagation Delta Functions [6 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fae496f31289341b3f883b8a6f61a02d",
     "grade": false,
     "grade_id": "ex_sigmoid_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this exercise we are first going to take the derivative of a famous activation function - the sigmoid function:\n",
    "\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "This function is commonly used because of its nice analytical properties: Its domain is $\\in[0,1]$, it is non-linear, strictly monotonous, continuous, differentiable and the derivative can be expressed in terms of the original function at the given point. This allows us to avoid redundant calculations. The sigmoid function is a special case of the more general *Logistic function* which can be found in many different fields: Biology, chemistry, economics, demography and recently most prominently: artificial neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a993b05bf60a0fbb050b88399672df3e",
     "grade": false,
     "grade_id": "ex_sigmoid_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Take the derivative $\\frac{\\partial \\sigma}{\\partial t}$ and (if possible) write the resulting expression in terms of $\\sigma(t)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d070973c87842692fbcd992338559ce8",
     "grade": true,
     "grade_id": "ex_sigmoid_a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5181e636cc42603f1374a79c12303e7e",
     "grade": false,
     "grade_id": "ex_sigmoid_b_intro",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Multilayer perceptrons (MLPs) can be regarded as a simple concatenation (and parallelization) of several perceptrons, each having a specified activation function $\\sigma$ and a set of weights $\\mathbf{w}_{ij}$. The idea that this can be done was discovered early after the invention of the perceptron, but people didn't really use it in practice because nobody really knew how to figure out the appropriate $\\mathbf{w}_{ij}$. The solution to this problem was the discovery of the backpropagation algorithm which consists of two steps: first propagating the input forward through the layers of the MLP and storing the intermediate results and then propagating the error backwards and adjusting the weights of the units accordingly.\n",
    "\n",
    "An updating rule for the output layer can be derived straightforward. The rules for the intermediate layers can be derived very similarly and only require a slight shift in perspective - the mathematics for that are however not in the standard toolkit so we are going to omit the calculations and refer you to the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88d82694ca98e9ba7d9e719a84c5b368",
     "grade": false,
     "grade_id": "ex_sigmoid_b_task",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We take the least-squares approach to derive the updating rule, i.e. we want to minimize the Loss function\n",
    "$$L = \\frac{1}{2}(y-t)^2$$\n",
    "where t is the given (true) label from the dataset and y is the (single) output produced by the MLP. To find the weights that minimize this expression we want to take the derivative of $L$ w.r.t. $\\mathbf{w}_{i}$ where we are now going to assume that the $\\mathbf{w}_{i}$ are the ones directly before the output layer:\n",
    "$$y = \\sigma\\left(\\sum_{k=1}^n \\mathbf{w}_{k}o_k\\right)$$\n",
    "Calculate $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}}$.\n",
    "\n",
    "*Hint*: Start here if you don't know what to do: $\\frac{\\partial L}{\\partial \\mathbf{w}_{i}} = \\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial \\mathbf{w}_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6273713a870f12b64691ca046b315387",
     "grade": true,
     "grade_id": "ex_sigmoid_b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.2",
    "jupytext_version": "1.4.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda475831754d4f421c8b9b78a005f24ea9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}